# Automated-testing-of-mobile-applications

## 自动化测试移动应用方向大作业

### 项目目录说明

#### 模型文件

&emsp;&emsp;yolo-obj_last.weights文件是yolov3模型迭代训练20000轮的模型文件

&emsp;&emsp;链接：https://pan.baidu.com/s/1OAQjjn2EKT2j6dPQXlGl2g 

&emsp;&emsp;提取码：wml6 

#### 第一阶段标注的数据集

&emsp;&emsp;解压TXTFiles.zip，会得到标注坐标和类别的TXT文件，TXT文件名对应初始提供的图片的文件名，标注的数据集用于yolov3模型的训练。

#### 数据解析

&emsp;&emsp;ParseData目录中放置的是数据解析相关的文件：

&emsp;&emsp;&emsp;&emsp;- ATMobile2020-1

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;目录中放置的是初始提供的图片文件、JSON文件，以及数据解析过程中生成的CSV文件（均经过压缩操作）

&emsp;&emsp;&emsp;&emsp;- MyYolo

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- Annotations

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;目录中放置的是标注好坐标和类别的TXT文件（经过压缩操作）

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- ImageSets

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- Main

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- test.txt

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- 记录用于测试的文件位置

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- train.txt

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- 记录用于训练的文件位置

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- trainval.txt

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- 记录用于验证训练的文件位置

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- val.txt

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- 记录用于验证的文件位置

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- JPEGImages

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;目录中放置的是初始提供的图片文件（经过压缩操作）

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- csv_to_txt.py

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;用于将记录控件信息的CSV文件转为只记录坐标和类别信息的TXT文件

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;- divide.py

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;用于划分训练集、测试集和验证集

&emsp;&emsp;&emsp;&emsp;- ParseJSON.py

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;用于解析初始提供的JSON文件，将每个图片对应的所有控件的信息保存为CSV文件

&emsp;&emsp;&emsp;&emsp;- SetType.py

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;用于输出所有CSV文件中包含的所有控件对应的class字段的信息（不重复），方便后续类别的设置以及CSV文件的过滤

&emsp;&emsp;&emsp;&emsp;- MarkType.py

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;用于标注每个CSV中所有控件的类别信息，具体标注规则见文件中的注释

&emsp;&emsp;&emsp;&emsp;- FilterCSV.py

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;用于过滤CSV文件中不重要的字段，具体的过滤规则见文件中的注释

&emsp;&emsp;&emsp;&emsp;- CheckIMG.py

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;用于将CSV中的控件以矩形框的形式打印到图片上，方便前续CSV的过滤以及类别的判定

#### darknet深度学习框架+yolov3算法

&emsp;&emsp;darknet目录放置的文件是实现yolov3算法的darknet深度学习框架，深度学习的具体流程以及与训练有关的内部文件的说明，参照demo视频。

#### 测试入口

&emsp;&emsp;RunModel.py是模型的测试入口，将其中的记录测试文件路径的文件更改为自定义的文件路径，并将前述百度云下载的模型放入darknet/build/darknet/x64/backup目录下，运行时每成功检测一张图片会直接在屏幕上显示标注的结果，关闭后可以自动进行下一张图片的识别，同时标注好的图片和标注信息会存储在darknet/build/darknet/x64/output目录下。

#### demo视频

&emsp;&emsp;demo视频针对数据解析阶段，主要介绍代码的运行顺序以及代码文件的作用，具体参照前述数据解析部分的目录结构详解；针对控件识别阶段，主要介绍模型配置文件的修改、模型训练部分过程以及模型测试，有不清楚的地方参照下述模型分析部分的内容。

### 模型分析

#### 模型结构

&emsp;&emsp;控件识别采用的是实现yolov3算法的darknet深度学习框架，利用第一阶段标注的数据集和初始提供的图片文件作为输入，最终实现输入目标检测图片的路径可以输出图片标注结果以及记录标注信息的TXT文件的效果。

&emsp;&emsp;使用darknet53.conv.74预训练模型，在无GPU加速的情况下，训练一轮的平均时间大约是50s，而得到一个loss值相对较低的模型所需的训练轮数理论上不应小于5000次（训练过程中观测得到的结论）；而配置Cuda和OpenCV后，开启GPU加速，训练一轮的平均时间缩短到3-5s，经过观测，模型经过8h训练了10000次左右，loss值保持在6上下，而训练20000次后，loss值降到4上下。

&emsp;&emsp;本次大作业控件识别采用的模型是迭代训练20000次的结果，我分别测试了训练1000次的模型测试结果、训练10000次的模型测试结果以及训练20000次的模型测试结果（下述实验验证只提供训练20000次的模型测试结果），发现相比于训练1000次的模型，训练10000次和训练20000次的模型识别准确度显然更高，但训练10000次的模型和训练20000次的模型识别准确度相差不大，而且考虑到训练次数过多会造成过拟合的结果，所以训练20000次的模型比较适合作为用于最终验证的模型。

&emsp;&emsp;实现yolov3算法的darknet模型默认的测试规则输出的仅仅是一张可视化的标注的图片，而作业要求同时输出描述控件的文件。经过查阅资料，修改yolov3的部分源码，最终实现在output目录中输出标注好的GUI截图以及描述GUI元素的文件。

#### 参考文献

- win10下YOLO v3训练自己的数据集 https://blog.csdn.net/kk123k/article/details/86696540

- 手把手教你在服务器上用YOLOv4训练和测试数据集（保姆级） https://blog.csdn.net/Creama_/article/details/106209388

- YOLOv2输出物体的位置坐标以及批次处理图片 https://blog.csdn.net/xiaomifanhxx/article/details/81003723

- 超详细教程：YOLO_V3（yolov3）训练自己的数据 https://blog.csdn.net/qq_21578849/article/details/84980298?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control

### 模型实验验证

#### 评估指标

- 通过控制台命令，检测测试集中的测试图片，将模型标注的图片与CheckIMG.py标注的图片进行对比，粗略进行模型评估

- 将训练好的yolo-obj_last.weights继续进行模型训练，在控制台上得到当前模型的准确评估

#### 验证结果

- 随机抽取训练集中的两张图片和测试集中的三张图片，观测到模型标注的图片标注部分完全正确，但存在不定量的未标注的部分；标注的类别以Image、Text和Button为主，原因是标注的数据集中这三类占的比重最大

- 当前模型的具体评估参数：总体Loss为4.565800，参与训练的图片的总量为320016

### 结果示例

> 左侧为模型输出标注结果，右侧为手工标注结果（训练前）

![](https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/AutoTesting/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-11-21%20125153.png)

![](https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/AutoTesting/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-11-21%20125314.png)

![](https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/AutoTesting/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-11-21%20125553.png)

![](https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/AutoTesting/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-11-21%20125648.png)

![](https://nandodu-blog.oss-cn-shanghai.aliyuncs.com/AutoTesting/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-11-21%20125745.png)

### 个人感想

&emsp;&emsp;一开始选择移动方向的大作业是因为以前用过appium做移动端APP的数据爬取，加上之前对OpenCV有所了解，所以对移动方向有一种自来熟的感觉。但其实我从一开始就知道这个作业难度很大，因为涉及到深度学习框架，虽然也可以选择传统计算机视觉技术，但我对深度学习的兴趣更大。

&emsp;&emsp;开始着手做这个作业其实距离DDL也只剩不到一周的时间了，当时很慌，因为完整看一遍作业要求之后觉得任务量很大，不是一周两周能赶出来的任务。准备开始做作业的时候，发现Mac升级系统之后Python的pip出现问题，花了将近两天的时间也没有解决，最后在stackoverflow求助，得到了可以使用anaconda的解决办法。当时觉得自己比较一根筋，明明有更容易的解决办法，却偏偏浪费了大量的时间，最后还是避开了一开始要解决的问题，但这个过程也很宝贵，也算是为后来训练模型的debug磨练耐心奠定了基础。

&emsp;&emsp;数据标注几乎没有遇到太大的阻碍，pandas库也因为上学期的数据科学系统大作业磨练得信手拈来，但在字段的解读上遇到了不小的困难，原始的JSON文件中很多字段不明所以，但最后通过结合图像标注的方式还是得以体面的解决。

&emsp;&emsp;控件识别我毫不犹豫地选择了深度学习的方式，而在YOLO和CenterNet的选取上举棋不定。可能是查到的YOLO的资料更多吧，最后选择了yolov3框架进行训练。

&emsp;&emsp;yolov3框架需要的标注数据集在网上查到的资料都是XML转换过来了，所以我花了不少时间捋清了yolov3到底需要的数据集格式是怎样的，然后将数据解析过程中生成的CSV文件转换成了用于训练模型的TXT文件。

&emsp;&emsp;一开始，我是在Mac上进行模型训练，配置和运行过程异常顺利，但是训练一轮的速度需要50s左右。我反复修改配置参数，还是没有办法提高训练速度。仔细地查找资料后，发现yolov3训练快的秘诀在于GPU加速，所以我在另一台Win10搭载1060MaxQ的笔记本上重新进行环境配置，一配置就是一整晚。Cuda版本的不匹配、VS2017编译失败、命令行进入模型训练之前跳出、内存溢出等等，debug花了几个小时忘记了，当天的晚饭都是在十二点多模型开始训练之后才应付一口的。但是模型开始训练的那一刻很激动，3-5s一轮的训练也给我带来了希望，我就让电脑足足运行了将近20个小时，得到了最终迭代训练20000轮的模型。

&emsp;&emsp;模型最终的识别效果还是比较准确的，训练集我统计了一下大概有16000个控件信息，但类别之间的数量差太悬殊，导致有的类别在测试时根本检测不到。而比较容易检测的Image、Text和Button控件，也会出现在GUI截图上标注不全的现象。比较惊喜的是能标注出来的控件基本都是准的。最后因为时间原因，在模型评估阶段只抽取了几张图片测试，没有写一个完整的计算准确率的文件，还是比较可惜的。赶工的作品效果达到这样也算是幸运女神的眷顾了吧。

&emsp;&emsp;整个大作业过程虽然煎熬，但收获很多。我也衷心感谢陈老师、房老师以及各位助教一学期以来的对我的指导和帮助，辛苦了！完结撒花~

